# ICAE - In-Context Autoencoder Project

è¿™ä¸€ä¸ªåŸºäº ICAE (In-Context Autoencoder) çš„æ”¹è¿›é¡¹ç›®ã€‚ICAE æ—¨åœ¨é€šè¿‡å°†é•¿ä¸Šä¸‹æ–‡å‹ç¼©åˆ°æœ‰é™çš„è®°å¿†æ§½ (Memory Slots) ä¸­ï¼Œä½¿å¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿå¤„ç†è¶…é•¿è¾“å…¥ã€‚

## ğŸ“ æ ¸å¿ƒæ–‡ä»¶è¯´æ˜

- **`modeling_icae_multi_span.py`**
  æ ¸å¿ƒæ¨¡å‹å®šä¹‰ã€‚åœ¨ Mistral ç­‰åŸºåº§æ¨¡å‹ä¸Šæ‰©å±•äº†è¯è¡¨ï¼Œå¢åŠ äº†è®°å¿† Tokenï¼Œå¹¶å®ç°äº†å‹ç¼©ï¼ˆç¼–ç ï¼‰å’Œé‡å»º/ç”Ÿæˆï¼ˆè§£ç ï¼‰çš„å‰å‘ä¼ æ’­é€»è¾‘ã€‚æ”¯æŒ LoRA å¾®è°ƒã€‚

- **`pretrain.py`**
  é¢„è®­ç»ƒè„šæœ¬ã€‚æ‰§è¡Œè‡ªç¼–ç ä»»åŠ¡ï¼ˆå‹ç¼©->é‡å»ºï¼‰å’Œè¯­è¨€å»ºæ¨¡ä»»åŠ¡ã€‚
  - **ç”¨æ³•**: `python pretrain.py --train_file train.jsonl --validation_file eval.jsonl ...`

- **`instruction_finetune.py`**
  æŒ‡ä»¤å¾®è°ƒè„šæœ¬ã€‚åœ¨é¢„è®­ç»ƒåŸºç¡€ä¸Šï¼Œè®©æ¨¡å‹å­¦ä¼šæ ¹æ® Input å’Œ Prompt åˆ©ç”¨è®°å¿† Token å›ç­”é—®é¢˜ã€‚

- **`training_utils.py`**
  æ•°æ®å¤„ç†å·¥å…·ã€‚åŒ…å«é’ˆå¯¹é¢„è®­ç»ƒå’Œå¾®è°ƒçš„ Tokenization é€»è¾‘ï¼ˆæ³¨æ„ï¼šåŒ…å«é’ˆå¯¹ç‰¹å®š Token ID çš„å¤„ç†ï¼Œå¦‚è¿ç§»æ¨¡å‹éœ€ä¿®æ”¹ï¼‰ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡
```bash
pip install torch transformers peft datasets
```

### 2. æ•°æ®å‡†å¤‡
å‡†å¤‡ JSONL æ ¼å¼çš„æ•°æ®ï¼š
- **é¢„è®­ç»ƒ**: `{"text": "é•¿æ–‡æœ¬..."}`
- **å¾®è°ƒ**: `{"input": "èƒŒæ™¯...", "prompt": "é—®é¢˜...", "answer": "ç­”æ¡ˆ..."}`

### 3. è¿è¡Œé¢„è®­ç»ƒ
```bash
python pretrain.py \
    --model_name_or_path mistralai/Mistral-7B-v0.1 \
    --output_dir ./output_pretrain \
    --train_file ./data/train.jsonl \
    --validation_file ./data/eval.jsonl \
    --fixed_mem_size 128 \
    --lora_r 128
```

## âš ï¸ å…¼å®¹æ€§è¯´æ˜ (Qwen/Llama)
æœ¬é¡¹ç›®é»˜è®¤é…ç½®é€‚é… **Mistral-7B-v0.1**ã€‚
è‹¥è¦è¿ç§»åˆ° **Qwen** æˆ– **Llama-3**ï¼Œè¯·åŠ¡å¿…ä¿®æ”¹ `training_utils.py` ä¸­çš„ Chat æ¨¡æ¿ç‰¹æ®Š Token IDï¼Œä»¥åŠ `modeling_icae_multi_span.py` ä¸­çš„ BOS/EOS ID å®šä¹‰ã€‚

## ğŸ”— å‚è€ƒ
åŸå§‹ ICAE é¡¹ç›®: [GitHub Link]
